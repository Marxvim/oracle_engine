{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperion Hub Code Generator\n",
    "## Oracle Developer\n",
    "c. marxvim 2024\n",
    "\n",
    "https://marxvim-1.gitbook.io/oracle-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%capture is a cell magic, but the cell body is empty.\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The specified config_list file 'OAI_CONFIG_LIST' does not exist.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import re\n",
    "from github import Github\n",
    "from github import Auth\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import autogen\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "open_ai_key = os.getenv(\"OPEN_AI_KEY\")\n",
    "git_token = Auth.Token(os.getenv(\"GITHUB_KEY\"))\n",
    "\n",
    "base_urls = None  # You can specify API base URLs if needed. eg: localhost:8000\n",
    "api_type = \"openai\"  # Type of API, e.g., \"openai\" or \"aoai\".\n",
    "api_version = None  # Specify API version if needed.\n",
    "\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': open_ai_key,\n",
    "    }\n",
    "]\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO LIST\n",
    "1. Use LangChain to connect to Anthropic, OpenAI, Azure models\n",
    "2. Use Chroma or Pinecode to store Vector Database Data from Github readme and ther `relevant user provided files`\n",
    "3. Use LangChain & OpenAI to generate Embeddings\n",
    "4. Use LangChain to implement RAG properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repos(auth_token=None):\n",
    "    auth = auth_token\n",
    "    github_repo = Github(auth=auth)\n",
    "    git_user = github_repo.get_user()\n",
    "    return git_user.get_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_gpt(prompt=\"\"):\n",
    "    openai.api_key = open_ai_key\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def extract_mermaid_script(gpt_response):\n",
    "    # Regex pattern to find content within ```mermaid ... ```\n",
    "    pattern = r\"```mermaid([\\s\\S]*?)```\"\n",
    "    match = re.search(pattern, gpt_response)\n",
    "    if match:\n",
    "        return match.group(1).strip()  # Returns only the script part\n",
    "    else:\n",
    "        return \"none\"  # If no script is found\n",
    "    \n",
    "def extract_readme(user_repos=None):\n",
    "    # for repo in user_repos:\n",
    "    #     print(repo.name)\n",
    "\n",
    "    aRepo = user_repos[10]\n",
    "    readme = aRepo.get_readme()\n",
    "    if readme:\n",
    "        # Print the README content\n",
    "        readme_content = readme.decoded_content.decode('utf-8')\n",
    "        return readme_content\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"create a python function that implements a binary search algorithm\"\n",
    "\n",
    "mermaid_script_prompt = (\n",
    "    \"Using Mermaid diagram syntax, create a script for the following task: \"\n",
    "    + user_prompt\n",
    "    + \". Please provide only the Mermaid script without any additional explanation, comments, or words. If a script cannot be generated for this task, simply return 'none'.\"\n",
    ")\n",
    "\n",
    "\n",
    "generated_mermaid_script = prompt_gpt(mermaid_script_prompt)\n",
    "mermaid_script = extract_mermaid_script(generated_mermaid_script)\n",
    "\n",
    "user_repos = get_repos(git_token)\n",
    "readme = extract_readme(user_repos)\n",
    "#print(readme)\n",
    "print(mermaid_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "We need a queue system that holds the IDs and names of incoming patients in a hospital. The function needs to be created using python\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure, we can model this scenario with a simple queue using a list in Python. We will define two functions: one function to add a patient to the queue and another function to remove a patient from the queue. When a patient is added, they are added to the end of the queue. When a patient is removed, the patient at the front of the queue is removed as per the First-In-First-Out (FIFO) property of a queue. \n",
      "\n",
      "Let's start by creating an empty list named `queue` to store our patients. A patient can be represented as a dictionary with keys `ID` and `name`.\n",
      "\n",
      "Here is the Python code that implements this model:\n",
      "\n",
      "```python\n",
      "queue = []\n",
      "\n",
      "def add_patient(ID, name):\n",
      "    patient = {\"ID\": ID, \"name\": name}\n",
      "    queue.append(patient)\n",
      "    print(f\"Patient {name} with ID {ID} has been added to the queue.\")\n",
      "\n",
      "def remove_patient():\n",
      "    if len(queue) == 0:\n",
      "        print(\"No patients in the queue.\")\n",
      "    else:\n",
      "        patient = queue.pop(0)\n",
      "        print(f\"Patient {patient['name']} with ID {patient['ID']} has been removed from the queue.\")\n",
      "\n",
      "#Let's add patients to the queue\n",
      "add_patient(101, \"John Doe\")\n",
      "add_patient(102, \"Jane Doe\")\n",
      "\n",
      "#Let's remove a patient from the queue\n",
      "remove_patient()\n",
      "```\n",
      "\n",
      "Make sure to execute this code by importing it to your Python environment.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mQuality_assurance_tester\u001b[0m (to chat_manager):\n",
      "\n",
      "This code first creates a list named \"queue\". \n",
      "\n",
      "Then, it defines two methods: \"add_patient\" and \"remove_patient\". \n",
      "\n",
      "- The \"add_patient\" method creates a new patient by including an \"ID\" and \"name\" into a dictionary, and then it appends this newly-created patient to the \"queue\" list. After appending the patient to the queue, the function displays a message confirming that the patient was added to the queue.\n",
      "\n",
      "- The \"remove_patient\" function first checks if there are any patients in the queue. If the queue is empty, it displays a message stating \"No patients in the queue.\" If there are patients in the queue, it pops (i.e., removes and returns) the first patient from the queue list, and then displays a message confirming that the patient was removed.\n",
      "\n",
      "To test these functions, the code adds two patients, \"John Doe\" and \"Jane Doe\", into the queue, and then removes one patient from the queue.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autogen.code_utils:execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Patient John Doe with ID 101 has been added to the queue.\n",
      "Patient Jane Doe with ID 102 has been added to the queue.\n",
      "Patient John Doe with ID 101 has been removed from the queue.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mQuality_assurance_tester\u001b[0m (to chat_manager):\n",
      "\n",
      "It seems that the code is working as expected. It successfully added two patients, \"John Doe\" and \"Jane Doe\", to the queue. Then, it removed the first patient \"John Doe\" from the queue, demonstrating the First-In-First-Out (FIFO) behavior of a queue. \n",
      "\n",
      "Is there anything else you want to add or any other functionality you want to implement?\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "llm_config = {\"config_list\": config_list, \"cache_seed\": 42}\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "   name=\"User_proxy\",\n",
    "   system_message=\"A human admin.\",\n",
    "   code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "   human_input_mode=\"NEVER\"\n",
    ")\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in software product ideas.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "qat = autogen.AssistantAgent(\n",
    "    name=\"Quality_assurance_tester\",\n",
    "    system_message=\"primarily focused on verifying that the software meets the specified quality standards, adheres to coding standards, and is free from defects or bugs\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, coder, qat], messages=[], max_round=12)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "user_proxy.initiate_chat(manager, message=user_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
